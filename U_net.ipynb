{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "将u-net换成cnn"
      ],
      "metadata": {
        "id": "OB2JzNN2RBLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data部分"
      ],
      "metadata": {
        "id": "xGvAxpJmWrN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAoqCURKPvbs",
        "outputId": "95475222-bdb5-4830-c37c-183b0ae7f7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 在Colab中导入自定义的包\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/My Drive/UTCI_prediction_all/model/Code-Yifan')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "# Custom Dataset Class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, spatial_data_dir, temporal_data_csv, output_data_dir, T_in, T_out, device='cuda'):\n",
        "        # Load the same four static spatial images\n",
        "        self.spatial_data = []\n",
        "        self.spatial_data_max = None\n",
        "        self.spatial_data_min = None\n",
        "\n",
        "        spatial_files = sorted(os.listdir(spatial_data_dir))[:4]  # Get the first 4 image files\n",
        "        for spatial_file in spatial_files:\n",
        "            spatial_image = tifffile.imread(os.path.join(spatial_data_dir, spatial_file))\n",
        "            spatial_image = torch.tensor(spatial_image).unsqueeze(0).to(device)  # Add channel dimension\n",
        "            spatial_image, self.spatial_data_max, self.spatial_data_min = self.maxminscaler_3d(spatial_image)  # normalize\n",
        "\n",
        "            # 直接对 Tensor 进行随机裁剪\n",
        "            spatial_image = TF.crop(spatial_image, top=0, left=0, height=64, width=64)\n",
        "            self.spatial_data.append(spatial_image)\n",
        "\n",
        "        # Stack into one tensor of shape [4, H, W] for static spatial information\n",
        "        self.spatial_data = torch.cat(self.spatial_data, dim=0)\n",
        "\n",
        "        # Load temporal data (e.g., weather data)\n",
        "        temporal_data_df = pd.read_csv(temporal_data_csv).iloc[:, 1:]  # Skip the first time stamp column\n",
        "        self.temporal_data = temporal_data_df.select_dtypes(include=[np.number]).fillna(0).astype(np.float32).values\n",
        "\n",
        "        # Correctly reshape the temporal data to have multiple samples if possible\n",
        "        num_samples_possible = (self.temporal_data.shape[0] // 336)  # Split into multiple samples, each with 336 time steps\n",
        "        self.temporal_data = self.temporal_data[:num_samples_possible * 336].reshape(-1, 336, 7)  # Reshape to [num_samples, 336, 7]\n",
        "\n",
        "        self.output_data_paths = [os.path.join(output_data_dir, f) for f in sorted(os.listdir(output_data_dir))]\n",
        "        self.device = device\n",
        "        self.T_in = T_in\n",
        "        self.T_out = T_out\n",
        "        self.utci_max = None  # 全局最大值，初始化为 None\n",
        "        self.utci_min = None  # 全局最小值，初始化为 None\n",
        "\n",
        "        # Debugging prints in __init__\n",
        "        print(\"CustomDataset initialized successfully.\")\n",
        "        print(f\"Number of spatial images: {len(spatial_files)}\")\n",
        "        print(f\"Temporal data shape after reshape: {self.temporal_data.shape}\")\n",
        "        print(f\"Number of output images (UTCI): {len(self.output_data_paths)}\")\n",
        "        print(f\"T_in: {T_in}, T_out: {T_out}\")\n",
        "\n",
        "        # 调用计算全局最大最小值的函数（修改点 1）\n",
        "        self.compute_utci_global_max_min()  # 计算全局最大最小值\n",
        "\n",
        "        # Calculate the maximum number of samples we can generate\n",
        "        self.num_samples = self.temporal_data.shape[0] * (336 - (self.T_in + self.T_out - 1))\n",
        "        print(f\"Calculated num_samples: {self.num_samples}\")\n",
        "\n",
        "        # If num_samples is not positive, raise an error\n",
        "        if self.num_samples <= 0:\n",
        "            raise ValueError(\"Not enough time steps to generate input and output sequences\")\n",
        "\n",
        "    def compute_utci_global_max_min(self):\n",
        "        \"\"\"计算整个 UTCI 数据集的全局最大值和最小值（修改点 2）\"\"\"\n",
        "        for output_file in self.output_data_paths:\n",
        "            output_data = tifffile.imread(output_file)\n",
        "            output_data_tensor = torch.tensor(output_data).unsqueeze(0).to(self.device)  # 加上通道维度\n",
        "            current_max = output_data_tensor.max().item()\n",
        "            current_min = output_data_tensor.min().item()\n",
        "\n",
        "            # 更新全局最大最小值\n",
        "            if self.utci_max is None or current_max > self.utci_max:\n",
        "                self.utci_max = current_max\n",
        "            if self.utci_min is None or current_min < self.utci_min:\n",
        "                self.utci_min = current_min\n",
        "\n",
        "        print(f\"全局 UTCI 最大值: {self.utci_max}, 最小值: {self.utci_min}\")  # 打印确认\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Determine the sample and time step index\n",
        "        sample_idx = idx // (336 - (self.T_in + self.T_out - 1))\n",
        "        time_idx = idx % (336 - (self.T_in + self.T_out - 1))\n",
        "\n",
        "        # Create the input spatial data sequence for T_in\n",
        "        spatial_data_seq = self.spatial_data.unsqueeze(-1).repeat(1, 1, 1, self.T_in)  # [4, H, W, T_in]\n",
        "        spatial_data_seq = spatial_data_seq.permute(1, 2, 0, 3)  # [H, W, 4, T_in]\n",
        "\n",
        "        # Load temporal data for T_in time steps\n",
        "        temporal_data = torch.tensor(self.temporal_data[sample_idx, time_idx:time_idx + self.T_in], dtype=torch.float32).to(self.device)\n",
        "\n",
        "        # Load output data (UTCI) for T_out time steps\n",
        "        output_data_list = []\n",
        "        for t in range(self.T_out):\n",
        "            output_data = tifffile.imread(self.output_data_paths[time_idx + self.T_in + t])\n",
        "            output_data = torch.tensor(output_data).unsqueeze(0).to(self.device)  # Add channel dimension\n",
        "\n",
        "            # 使用全局的最大最小值进行归一化（修改点 3）\n",
        "            output_data, _, _ = self.maxminscaler_3d(output_data, self.utci_max, self.utci_min)  # normalize\n",
        "\n",
        "            # 对 Tensor 进行随机裁剪\n",
        "            output_data = TF.crop(output_data, top=0, left=0, height=64, width=64)\n",
        "            output_data_list.append(output_data)\n",
        "\n",
        "        # Stack the output time steps to form [T_out, 1, H, W] and then permute to [H, W, 1, T_out]\n",
        "        output_data = torch.stack(output_data_list).permute(2, 3, 1, 0)\n",
        "\n",
        "        # Add the UTCI temporal data as the 5th channel to spatial_data_seq\n",
        "        # Load UTCI data (input for T_in time steps)\n",
        "        utci_input_list = []\n",
        "        for t in range(self.T_in):\n",
        "            utci_data = tifffile.imread(self.output_data_paths[time_idx + t])\n",
        "            utci_data = torch.tensor(utci_data).unsqueeze(0).to(self.device)  # Add channel dimension\n",
        "\n",
        "            # 使用全局的最大最小值进行归一化（修改点 4）\n",
        "            utci_data, _, _ = self.maxminscaler_3d(utci_data, self.utci_max, self.utci_min)  # normalize\n",
        "\n",
        "            # 对 Tensor 进行随机裁剪\n",
        "            utci_data = TF.crop(utci_data, top=0, left=0, height=64, width=64)\n",
        "            utci_input_list.append(utci_data)\n",
        "\n",
        "        # Stack the UTCI input time steps to form [T_in, 1, H, W] and then permute to [H, W, 1, T_in]\n",
        "        utci_input = torch.stack(utci_input_list).permute(2, 3, 1, 0)  # [H, W, 1, T_in]\n",
        "\n",
        "        # Concatenate the UTCI input data to spatial_data_seq\n",
        "        spatial_data_seq = torch.cat([spatial_data_seq, utci_input], dim=2)  # [H, W, 5, T_in]\n",
        "\n",
        "        # Print the shape of the combined input data (spatial + UTCI)\n",
        "        print(\"Combined input data shape (expected [H, W, 5, T_in]):\", spatial_data_seq.shape)\n",
        "\n",
        "        # Check the shape of output_data\n",
        "        print(\"Output data shape (expected [H, W, 1, T_out]):\", output_data.shape)\n",
        "\n",
        "        # Return the full spatial data (now with 5 channels), output data, and temporal data\n",
        "        return spatial_data_seq, output_data, temporal_data\n",
        "\n",
        "    def maxminscaler_3d(self, tensor_3d, scaler_max=None, scaler_min=None, range=(0, 1)):\n",
        "        \"\"\"归一化函数，使用全局最大最小值（如果提供）（修改点 5）\"\"\"\n",
        "        if scaler_max is None:\n",
        "            scaler_max = tensor_3d.max()\n",
        "        if scaler_min is None:\n",
        "            scaler_min = tensor_3d.min()\n",
        "\n",
        "        X_std = (tensor_3d - scaler_min) / (scaler_max - scaler_min)\n",
        "        X_scaled = X_std * (range[1] - range[0]) + range[0]\n",
        "        return X_scaled, scaler_max, scaler_min\n",
        "\n",
        "# Paths to directories and files\n",
        "spatial_data_dir = r'/content/drive/MyDrive/UTCI_prediction_all/data/data_test9/spatial_images_256'\n",
        "temporal_data_csv = r'/content/drive/MyDrive/UTCI_prediction_all/data/data_test9/Yifan_updated_data_with_timestamps.csv'\n",
        "output_data_dir = r'/content/drive/MyDrive/UTCI_prediction_all/data/data_test9/output_images_256'\n",
        "\n",
        "# Define T_in and T_out\n",
        "T_in = 24   # Input time steps\n",
        "T_out = 24  # Output time steps\n",
        "\n",
        "# 创建 Dataset 并应用随机裁剪\n",
        "batch_size = 4  # Example batch size\n",
        "dataset = CustomDataset(spatial_data_dir, temporal_data_csv, output_data_dir, T_in, T_out, device='cuda')\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "rEj6Ymlolg-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data loader and visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image(image, title=\"\"):\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for spatial_data, output_data, temporal_data in dataloader:\n",
        "    print(\"Loaded spatial data shape:\", spatial_data.shape)  # [batch_size, H, W, C, T_in]\n",
        "    print(\"Loaded output data shape:\", output_data.shape)    # [batch_size, H, W, C, T_out]\n",
        "    print(\"Loaded temporal data shape:\", temporal_data.shape)\n",
        "\n",
        "    # Visualize first sample's spatial data\n",
        "    for i in range(spatial_data.shape[4]):\n",
        "        for c in range(spatial_data.shape[3]):\n",
        "            show_image(spatial_data[0, :, :, c, i].cpu().numpy(), title=f\"Spatial Data Channel {c} - Time Step {i}\")\n",
        "\n",
        "    # Visualize first sample's output data\n",
        "    for i in range(output_data.shape[4]):\n",
        "        show_image(output_data[0, :, :, 0, i].cpu().numpy(), title=f\"Output Data - Time Step {i}\")\n",
        "\n",
        "    break  # Only process one batch for visualization"
      ],
      "metadata": {
        "id": "o3vprme2lq-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "将卷积网络修改成u-net网络"
      ],
      "metadata": {
        "id": "4ByX91ABQuuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "P03g6htzl7ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNetSpatialFeatures(nn.Module):\n",
        "    def __init__(self, kernel_size=3, batch_size=4, seq_len=24, height=64, width=64, channels=5):\n",
        "        \"\"\"\n",
        "        通过U-Net架构捕获每个时刻的空间依赖特性。\n",
        "        :param kernel_size: 卷积核大小\n",
        "        :param batch_size: 批量大小\n",
        "        :param seq_len: 时间序列长度\n",
        "        :param height: 输入的空间网格高度\n",
        "        :param width: 输入的空间网格宽度\n",
        "        :param channels: 输入通道数\n",
        "        \"\"\"\n",
        "        super(UNetSpatialFeatures, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # U-Net编码器部分（下采样）\n",
        "        self.encoder_conv1 = nn.Conv2d(self.channels, 64, kernel_size=kernel_size, padding=1)\n",
        "        self.encoder_conv2 = nn.Conv2d(64, 128, kernel_size=kernel_size, padding=1)\n",
        "        self.encoder_conv3 = nn.Conv2d(128, 256, kernel_size=kernel_size, padding=1)\n",
        "        self.encoder_conv4 = nn.Conv2d(256, 512, kernel_size=kernel_size, padding=1)\n",
        "\n",
        "        # 最大池化层用于下采样\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # U-Net解码器部分（上采样）\n",
        "        self.decoder_conv4 = nn.Conv2d(512, 256, kernel_size=kernel_size, padding=1)\n",
        "        self.decoder_conv3 = nn.Conv2d(256, 128, kernel_size=kernel_size, padding=1)\n",
        "        self.decoder_conv2 = nn.Conv2d(128, 64, kernel_size=kernel_size, padding=1)\n",
        "        self.decoder_conv1 = nn.Conv2d(64, 1, kernel_size=kernel_size, padding=1)\n",
        "\n",
        "        # 上采样层\n",
        "        self.upsample = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        输入形状：[batch_size, height, width, channels, seq_len]\n",
        "        输出形状：[batch_size, seq_len, 1, height, width]\n",
        "        \"\"\"\n",
        "        output = torch.empty(input.shape[0], self.seq_len, 1, self.height, self.width).cuda()\n",
        "        for index in range(self.seq_len):\n",
        "            x_input = input[:, :, :, :, index]  # 选择第 index 个时间步\n",
        "            x_input = x_input.permute(0, 3, 1, 2)  # 转换为 [batch_size, channels, height, width]\n",
        "\n",
        "            # 编码器部分\n",
        "            enc1 = self.relu(self.encoder_conv1(x_input))  # 第一层卷积\n",
        "            enc2 = self.relu(self.encoder_conv2(self.maxpool(enc1)))  # 第二层卷积\n",
        "            enc3 = self.relu(self.encoder_conv3(self.maxpool(enc2)))  # 第三层卷积\n",
        "            enc4 = self.relu(self.encoder_conv4(self.maxpool(enc3)))  # 第四层卷积\n",
        "\n",
        "            # 解码器部分\n",
        "            dec4 = self.relu(self.decoder_conv4(self.upsample(enc4)))  # 上采样并卷积\n",
        "            dec3 = self.relu(self.decoder_conv3(self.upsample(dec4)))  # 上采样并卷积\n",
        "            dec2 = self.relu(self.decoder_conv2(self.upsample(dec3)))  # 上采样并卷积\n",
        "            dec1 = self.decoder_conv1(dec2)  # 最后一层输出单通道\n",
        "\n",
        "            # 保存U-Net输出\n",
        "            output[:, index, :, :, :] = dec1\n",
        "        return output"
      ],
      "metadata": {
        "id": "FhMvVz4YPzzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "输入数据的形状：U-Net 模块将提取空间特征，输出形状为 [batch_size, seq_len, 1, height, width]，而 LSTM 处理时间序列的输入通常是 [batch_size, seq_len, channels * height * width]。因此你需要在 U-Net 输出和 LSTM 输入之间进行形状调整。\n",
        "\n",
        "集成结构：一般来说，U-Net 用于空间特征提取，LSTM 用于时间特征建模。所以可以考虑将每个时间步的空间特征通过 U-Net 处理后，再输入到 LSTM 中进行时间依赖建模。\n",
        "\n",
        "调整数据格式：在将空间特征传递给 LSTM 之前，需要将 U-Net 的输出从 [batch_size, seq_len, 1, height, width] 转换为 LSTM 可以处理的格式 [batch_size, seq_len, height * width]。"
      ],
      "metadata": {
        "id": "NV2I9rlkm_eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNetSpatialFeatures(nn.Module):\n",
        "    def __init__(self, kernel_size=3, batch_size=4, seq_len=24, height=64, width=64, channels=5):\n",
        "        super(UNetSpatialFeatures, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # U-Net编码器部分\n",
        "        self.encoder_conv1 = nn.Conv2d(self.channels, 64, kernel_size=kernel_size, padding=1)\n",
        "        self.encoder_conv2 = nn.Conv2d(64, 128, kernel_size=kernel_size, padding=1)\n",
        "        self.encoder_conv3 = nn.Conv2d(128, 256, kernel_size=kernel_size, padding=1)\n",
        "        self.encoder_conv4 = nn.Conv2d(256, 512, kernel_size=kernel_size, padding=1)\n",
        "\n",
        "        # 最大池化层\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # U-Net解码器部分\n",
        "        self.decoder_conv4 = nn.Conv2d(512, 256, kernel_size=kernel_size, padding=1)\n",
        "        self.decoder_conv3 = nn.Conv2d(256, 128, kernel_size=kernel_size, padding=1)\n",
        "        self.decoder_conv2 = nn.Conv2d(128, 64, kernel_size=kernel_size, padding=1)\n",
        "        self.decoder_conv1 = nn.Conv2d(64, 1, kernel_size=kernel_size, padding=1)\n",
        "\n",
        "        self.upsample = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = torch.empty(input.shape[0], self.seq_len, 1, self.height, self.width).cuda()\n",
        "        for index in range(self.seq_len):\n",
        "            x_input = input[:, :, :, :, index]  # 提取每个时间步的数据\n",
        "            x_input = x_input.permute(0, 3, 1, 2)  # [batch_size, channels, height, width]\n",
        "\n",
        "            # 编码器部分\n",
        "            enc1 = self.relu(self.encoder_conv1(x_input))\n",
        "            enc2 = self.relu(self.encoder_conv2(self.maxpool(enc1)))\n",
        "            enc3 = self.relu(self.encoder_conv3(self.maxpool(enc2)))\n",
        "            enc4 = self.relu(self.encoder_conv4(self.maxpool(enc3)))\n",
        "\n",
        "            # 解码器部分\n",
        "            dec4 = self.relu(self.decoder_conv4(self.upsample(enc4)))\n",
        "            dec3 = self.relu(self.decoder_conv3(self.upsample(dec4)))\n",
        "            dec2 = self.relu(self.decoder_conv2(self.upsample(dec3)))\n",
        "            dec1 = self.decoder_conv1(dec2)  # 输出单通道\n",
        "\n",
        "            output[:, index, :, :, :] = dec1\n",
        "        return output\n",
        "\n",
        "\n",
        "class Convolution_LSTM(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, num_layer=2, batch_size=4, height=64, width=64, T_out=24):\n",
        "        super(Convolution_LSTM, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_layer = num_layer\n",
        "        self.batch_size = batch_size\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.T_out = T_out\n",
        "\n",
        "        # LSTM定义\n",
        "        self.lstm = nn.LSTM(input_size=self.input_channels * self.height * self.width,\n",
        "                            hidden_size=self.hidden_channels * self.height * self.width,\n",
        "                            num_layers=self.num_layer, batch_first=True)\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.hidden2out_1 = nn.Linear(self.hidden_channels * self.height * self.width,\n",
        "                                      self.input_channels * self.height * self.width)\n",
        "\n",
        "    def initialize_parameters(self, batch_size):\n",
        "        return (torch.zeros(self.num_layer, batch_size, self.hidden_channels * self.height * self.width).cuda(),\n",
        "                torch.zeros(self.num_layer, batch_size, self.hidden_channels * self.height * self.width).cuda())\n",
        "\n",
        "    def forward(self, input):\n",
        "        # 将输入展平为 [batch_size, seq_len, height * width]，适应 LSTM 输入\n",
        "        input = input.view(input.shape[0], input.shape[1], -1)  # [batch_size, seq_len, H*W]\n",
        "\n",
        "        # 初始化 LSTM 隐藏层参数\n",
        "        h0, c0 = self.initialize_parameters(input.shape[0])\n",
        "\n",
        "        # LSTM 前向传播\n",
        "        outputs, _ = self.lstm(input, (h0, c0))\n",
        "\n",
        "        # 只保留最后的 T_out 个时间步的输出\n",
        "        outputs = outputs[:, -self.T_out:, :]\n",
        "        outputs = self.hidden2out_1(outputs)\n",
        "        outputs = self.tanh(outputs)\n",
        "\n",
        "        # 将输出形状调整为 [batch_size, T_out, 1, height, width]\n",
        "        output = outputs.view(input.shape[0], self.T_out, 1, self.height, self.width)\n",
        "        return output\n",
        "\n",
        "\n",
        "class UNetLSTM(nn.Module):\n",
        "    def __init__(self, unet, conv_lstm):\n",
        "        super(UNetLSTM, self).__init__()\n",
        "        self.unet = unet\n",
        "        self.conv_lstm = conv_lstm\n",
        "\n",
        "    def forward(self, input):\n",
        "        # U-Net提取空间特征\n",
        "        spatial_features = self.unet(input)\n",
        "\n",
        "        # 调整形状以适应 LSTM 输入\n",
        "        spatial_features = spatial_features.view(spatial_features.size(0), spatial_features.size(1), -1)\n",
        "\n",
        "        # 将空间特征输入LSTM进行时间序列建模\n",
        "        output = self.conv_lstm(spatial_features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "qreOa4ehnAlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Regular_Convolution_LSTM(nn.Module):\n",
        "    def __init__(self, kernel_size, batch_size=4, T_in=24, T_out=24, height=64, width=64, channels=5):\n",
        "        \"\"\"\n",
        "        结合空间卷积和LSTM模块来捕捉时空依赖性，输入为时间序列，生成多步预测输出。\n",
        "        :param kernel_size: 卷积核大小\n",
        "        :param batch_size: 批量大小\n",
        "        :param T_in: 输入的时间步长\n",
        "        :param T_out: 输出的时间步长\n",
        "        :param height: 空间网格的高度\n",
        "        :param width: 空间网格的宽度\n",
        "        :param channels: 输入通道数\n",
        "        \"\"\"\n",
        "        super(Regular_Convolution_LSTM, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.batch_size = batch_size\n",
        "        self.T_in = T_in\n",
        "        self.T_out = T_out\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "\n",
        "        # 卷积部分提取空间特征\n",
        "        self.conv_module = Extraction_spatial_features(kernel_size=self.kernel_size, batch_size=self.batch_size,\n",
        "                                                       seq_len=self.T_in, height=self.height, width=self.width,\n",
        "                                                       channels=self.channels).cuda()\n",
        "\n",
        "        # LSTM部分捕捉时间依赖性并生成 T_out 输出\n",
        "        self.convlstm = Convolution_LSTM(input_channels=1, hidden_channels=1,\n",
        "                                         num_layer=2, batch_size=self.batch_size, height=self.height,\n",
        "                                         width=self.width, T_out=self.T_out).cuda()\n",
        "\n",
        "        # 激活函数\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 卷积模块提取空间特征，形状为 [batch_size, T_in, 1, height, width]\n",
        "        output_spatial = self.conv_module(x)\n",
        "\n",
        "        # 调整形状为 [batch_size, T_in, height * width]，以适应LSTM输入\n",
        "        batch_size, seq_len, _, height, width = output_spatial.size()\n",
        "        output_spatial_flat = output_spatial.view(batch_size, seq_len, -1)  # 展平为 [batch_size, T_in, height * width]\n",
        "\n",
        "        # LSTM模块捕捉时间依赖性，并输出 T_out 个时间步的预测结果\n",
        "        output_lstm = self.convlstm(output_spatial_flat)\n",
        "\n",
        "        # 激活函数\n",
        "        output = self.tanh(output_lstm)\n",
        "\n",
        "        # 将输出形状从 [batch_size, T_out, 1, height, width] 调整为 [batch_size, height, width, 1, T_out]\n",
        "        output = output.permute(0, 3, 4, 2, 1)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "z4CPnJ_3oFG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for spatial_data, output_data, temporal_data in dataloader:\n",
        "    # 打印加载的数据形状\n",
        "    print(\"Loaded spatial data shape:\", spatial_data.shape)  # [batch_size, h, w, channels, T_in]\n",
        "    print(\"Loaded output data shape:\", output_data.shape)    # [batch_size, h, w, 1, T_out]\n",
        "    print(\"Loaded temporal data shape:\", temporal_data.shape)  # 暂时不处理 temporal_data\n",
        "\n",
        "    # 实例化模型\n",
        "    model = Regular_Convolution_LSTM(kernel_size=3, batch_size=4, T_in=24, T_out=24, height=64, width=64, channels=5).cuda()\n",
        "\n",
        "    # 确保输入的形状为 [batch_size, channels, height, width, T_in]\n",
        "    spatial_data = spatial_data.permute(0, 3, 1, 2, 4).cuda()  # 调整维度顺序为 [batch_size, channels, height, width, T_in]\n",
        "\n",
        "    # 前向传播测试\n",
        "    output = model(spatial_data)\n",
        "\n",
        "    # 打印输出的形状，验证是否符合预期 [batch_size, height, width, channels, T_out]\n",
        "    print(\"Model output shape:\", output.shape)  # 期望输出形状为 [batch_size, height, width, channels, T_out]\n",
        "\n",
        "    break  # 只运行一次进行测试"
      ],
      "metadata": {
        "id": "hZQPft_YoU4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}